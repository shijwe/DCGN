{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCGN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning approach for cancer subtype classification using high-dimensional gene expression data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.layers import Conv2D,BatchNormalization,Activation,MaxPool2D,Dropout,Flatten,Dense\n",
    "from tensorflow.keras import Model,datasets\n",
    "import os\n",
    "\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = scipy.io.loadmat('/data/BRCASmote.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(mat['data']).astype(np.float32)\n",
    "data = data.T\n",
    "tag = np.array(mat['targets']).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4221, 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4221, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output the retrieved data and labels, and make sure the data is (number of samples, feature dimension) and the labels are (number of samples, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform feature standardization\n",
    "Feature normalization is performed on the gene expression data before model training so that all features in the dataset have zero mean and unit variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.mean(axis=1)\n",
    "data = data - data.mean(axis=0,keepdims=True)\n",
    "data = data / np.sqrt(data.var(axis=0,keepdims = True))\n",
    "data.var(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "array([1.0000001 , 0.99999964, 0.9999994 , ..., 0.9999986 , 1.0000017 ,\n",
    "       1.0000012 ], dtype=float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the random seed to a certain value for reproducibility and disrupt the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7)\n",
    "np.random.shuffle(data)\n",
    "np.random.seed(7)\n",
    "np.random.shuffle(tag)\n",
    "tf.random.set_seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gelu activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "            return 0.5 * x * (1 + tf.tanh(tf.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "DCGN combining CNN and BiGRU, aiming to achieve nonlinear dimensionality reduction in the process of learning important features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure the  shape of the build function is (number of samples, feature dimension) and change the number of samples according to different data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras import Input\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import GRU,Conv2D,BatchNormalization,Activation,MaxPool2D,Dropout,Flatten,Dense\n",
    "from tensorflow.keras import Model,datasets\n",
    "import os\n",
    "\n",
    "class MyModel(tf.keras.Model):\n",
    "    \n",
    "\n",
    "    def __init__(self):\n",
    "        super (MyModel,self).__init__()\n",
    "        self.f5 = tf.keras.layers.Dense(1024,activation = gelu,kernel_regularizer=tf.keras.regularizers.l2())\n",
    "        \n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters = 128,kernel_size = 3,padding='same',strides = 2,activation=gelu)\n",
    "        self.max1 = tf.keras.layers.MaxPool2D(pool_size = (2,2),strides = 2)\n",
    "        self.l1 = tf.keras.layers.Bidirectional(GRU(64,return_sequences = True,bias_regularizer=tf.keras.regularizers.l2(1e-4),\n",
    "                                         activity_regularizer=tf.keras.regularizers.l2(1e-5)))\n",
    "        self.conv3 = tf.keras.layers.Conv2D(filters = 64,kernel_size = 3,padding='same',strides = 2,activation=gelu)\n",
    "        self.max2 = tf.keras.layers.MaxPool2D(pool_size = (2,2),strides = 2)\n",
    "        \n",
    "        \n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.f1 = tf.keras.layers.Dense(128,activation =gelu,kernel_regularizer=tf.keras.regularizers.l2())\n",
    "        self.f2 = tf.keras.layers.Dense(64,activation =gelu,kernel_regularizer=tf.keras.regularizers.l2())\n",
    "        self.d2 = tf.keras.layers.Dropout(rate=0.6)\n",
    "        self.f3 = tf.keras.layers.Dense(32,activation =gelu,kernel_regularizer=tf.keras.regularizers.l2())\n",
    "        self.d3 = tf.keras.layers.Dropout(rate=0.7)\n",
    "        self.f4 = tf.keras.layers.Dense(10)\n",
    "        \n",
    "    def call(self,x):\n",
    "        x = self.f5(x)\n",
    "        x = Reshape((32,32,1))(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.max1(x)\n",
    "        x = Reshape((128,64))(x)\n",
    "        x = self.l1(x)\n",
    "        \n",
    "        x = Reshape((128,128,1))(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.max2(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.f1(x)\n",
    "        x = self.f2(x)\n",
    "        x = self.d2(x)\n",
    "        x = self.f3(x)\n",
    "        x = self.d3(x)\n",
    "        y = self.f4(x)\n",
    "        return y\n",
    "    \n",
    "    \n",
    "model = MyModel()\n",
    "model.build(input_shape=(1010,20000))\n",
    "model.call(Input(shape=(20000,)))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and validation\n",
    "Import multi-Category evaluation packages and set the loss function.Define gradient descent to minimize the classification loss during the training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score,confusion_matrix\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import hamming_loss\n",
    "import os\n",
    "\n",
    "optimizer = keras.optimizers.Adam()\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "val_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x, training=True)\n",
    "        loss_value = loss_fn(y, logits)\n",
    "        y_pre = tf.argmax(logits,axis=1)\n",
    "        y_pre = tf.reshape(y_pre,y.shape)\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    f1 = f1_score(y, y_pre, average='weighted' )\n",
    "    p = precision_score(y, y_pre, average='weighted')\n",
    "    r = recall_score(y, y_pre, average='weighted')\n",
    "    train_acc_metric.update_state(y, logits)\n",
    "    return f1,p,r\n",
    "\n",
    "\n",
    "def val_step(x, y):\n",
    "    val_logits = model(x, training=False)\n",
    "    val_y_pre =tf.argmax(val_logits,axis=1)\n",
    "    val_y_pre =tf.reshape(val_y_pre,y.shape)\n",
    "    f1 = f1_score(y, val_y_pre, average='weighted' )\n",
    "    p = precision_score(y, val_y_pre, average='weighted')\n",
    "    r = recall_score(y, val_y_pre, average='weighted')\n",
    "    cm = confusion_matrix(y,val_y_pre)\n",
    "    kappa = cohen_kappa_score(y,val_y_pre)\n",
    "    ham = hamming_loss(y,val_y_pre)\n",
    "    val_acc_metric.update_state(y, val_logits)\n",
    "    return f1,p,r,kappa,ham"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "Divide the dataset, perform the training and validation steps in 100 epochs, and finally perform the testing step. Note that the size of different datasets is not the same. The code needs to be modified when changing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "epos = 100\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((data[:3376],tag[:3376])).batch(256)\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((data[3376:4198],tag[3376:4198])).batch(256)\n",
    "test_x = data[4198:]\n",
    "test_y = tag[4198:]\n",
    "for epo in range(epos):\n",
    "    print(\"\\nStart of epoch %d\" % (epo,))\n",
    "    for x_batch_train, y_batch_train in train_ds:\n",
    "        train_f1,train_pre,train_reca=train_step(x_batch_train, y_batch_train)\n",
    "    train_acc = train_acc_metric.result()\n",
    "\n",
    "    train_acc_metric.reset_states()\n",
    "\n",
    "    for x_batch_val, y_batch_val in val_ds:\n",
    "        val_f1,val_pre,val_reca,kappa,ham = val_step(x_batch_val, y_batch_val)\n",
    "    val_acc = val_acc_metric.result()\n",
    "    \n",
    "    val_acc_metric.reset_states()\n",
    "    print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
    "    print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
    "    print(\"Training pre over epoch: %.4f\" % (float(train_pre),))\n",
    "    print(\"Validation pre: %.4f\" % (float(val_pre),))\n",
    "    print(\"Training reca over epoch: %.4f\" % (float(train_reca),))\n",
    "    print(\"Validation reca: %.4f\" % (float(val_reca),))\n",
    "    print(\"Training f1 over epoch: %.4f\" % (float(train_f1),))\n",
    "    print(\"Validation f1: %.4f\" % (float(val_f1),))\n",
    "    print(\"Validation kappa: %.4f\" % (float(kappa),))\n",
    "    print(\"Validation 海明距离: %.4f\" % (float(ham),))\n",
    "#Define model checkpoint and save the best model\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath = '/weights.{epo:02d}.hdf5' , monitor='val_f1', verbose=1,save_weights_only=True，\n",
    "                                                save_best_only=True, mode='max')\n",
    "\n",
    "model.save_weights('/')#File path to save the best model weights\n",
    "model.load_weights('/')#Load the weights saved above\n",
    "test_pre = model.predict(test_x)\n",
    "test_acc = train_acc_metric(test_y,test_pre)\n",
    "print(\"The training process is over and the testing begins...\")\n",
    "print(\"Test acc over training: %.4f\" % (float(test_acc),))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start of epoch 38\n",
    "Training acc over epoch: 0.9406\n",
    "Validation acc: 0.9356\n",
    "Training pre over epoch: 0.9519\n",
    "Validation pre: 0.9363\n",
    "Training reca over epoch: 0.9500\n",
    "Validation reca: 0.9356\n",
    "Training f1 over epoch: 0.9501\n",
    "Validation f1: 0.9355\n",
    "Validation kappa: 0.9033\n",
    "Validation 海明距离: 0.0644\n",
    "\n",
    "Start of epoch 39\n",
    "Training acc over epoch: 0.9505\n",
    "Validation acc: 0.9356\n",
    "Training pre over epoch: 0.9766\n",
    "Validation pre: 0.9363\n",
    "Training reca over epoch: 0.9250\n",
    "Validation reca: 0.9356\n",
    "Training f1 over epoch: 0.9478\n",
    "Validation f1: 0.9355\n",
    "Validation kappa: 0.9033\n",
    "Validation 海明距离: 0.0644\n",
    "\n",
    "Start of epoch 40\n",
    "Training acc over epoch: 0.9406\n",
    "Validation acc: 0.9406\n",
    "Training pre over epoch: 0.9766\n",
    "Validation pre: 0.9413\n",
    "Training reca over epoch: 0.9500\n",
    "Validation reca: 0.9406\n",
    "Training f1 over epoch: 0.9619\n",
    "Validation f1: 0.9407\n",
    "Validation kappa: 0.9107\n",
    "Validation 海明距离: 0.0594"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
